### 熵相关

#### 信息量

与信息发生的概率成反比
$$
I(x) = -\sum_{i=1}^n log(P(x_i))
$$

#### 信息熵

也叫熵，是随机变量不确定性的度量，不确定性越大，熵值越大；若随机变量退化成定值，熵为0（概率总是在 0-1 之间，log后会小于 0，取负号以后熵就是正数）
$$
H(p) = -\sum_{i=1}^np(x_i)*log(p(x_i))   \\ = \sum_{i=1}^np(x_i)*log\frac{1}{p(x_i)}
$$

#### 最大熵

系统中事件发生的概率满足一切已知约束条件，不对任何未知信息做假设，也就是对于未知的，当作等概率处理。

#### 相对熵（KL散度）

衡量两个概率分布之间的差异。熵的大小可以度量编码 p 最少需要多少空间，而 KL 散度则是衡量使用一个概率分布代表另一个概率分布所损失的信息量。因为不符合对称性，所以 KL 散度不是距离。
$$
D_{KL}(p||q) = \sum_{i=1}^{n}p(x_i)*log\frac{p(x_i)}{q(x_i)}
$$
在机器学习中，常使用 p(x) 来代表样本的真实分布，q(x) 来表示模型所预测的分布，比如在一个三分类任务中（猫马狗分类），x1, x2, x3 分别代表猫、马、狗，例如，一张猫的图片的真实分布p(x) = [1, 0, 0]，预测分布为 q(x) = [0.7, 0.2, 0.1]，计算KL散度：
$$
D_{KL}(p||q) = \sum_{i=1}^{n}p(x_i)*log\frac{p(x_i)}{q(x_i)}\\= 1 * log\frac{1}{0.7} + 0 * log\frac{0}{0.2} + 0 * log\frac{0}{0.2} = 0.36
$$
KL 散度越小，表示 p(x) 与 q(x) 的分布更加接近，可以通过反复训练 q(x) 来使 q(x) 的分布逼近 p(x)。

#### 交叉熵

等于 KL 散度+信息熵（在具体 ML 任务中是常量），常用来作为损失函数。
$$
H(p,q) = -\sum_{i=1}^np(x_i)*log(q(x_i)) \\ = \sum_{i=1}^np(x_i)*log\frac{1}{q(x_i)}
$$
在机器学习训练网络时，输入数据与标签常常已经确定，那么真实概率分布 p(x) 也就确定下来了，所以信息熵在这里就是一个常量。由于 KL 散度的值表示真实概率分布 p(x) 与预测概率分布 q(x) 之间的差异，值越小表示预测的结果越好，所以需要最小化 KL 散度，而交叉熵等于 KL 散度加上一个常量（信息熵），且公式相比 KL 散度更加容易计算，所以在机器学习中常常使用交叉熵损失函数来计算 loss。

### loss 相关

#### softmax函数

softmax 函数将输出值变成值为正且和为1的概率分布，softmax 回归适用于分类（离散）问题。softmax 回归跟线性回归一样将输入特征与权重做线性叠加，不同在于 softmax 回归的输出值个数等于标签里的类别数（线性回归是一个），更适合离散值的预测和训练。

假设一个样本，它有 n 种类别，在进入 softmax 之前有一层全连接层，它的输出是 $ a_1, a_2,...,a_n$，那么这个样本属于类别 i 的概率通过 softmax 公式计算为：
$$
p(i) = \frac{e^{a_i}}{\sum_{j=1}^{n}e^{a_j}}, \forall i \in 1...n
$$
上式可以保证属于各类别的概率和为1。举个例子，如果全连接层的输出是 [0.1, 0.2, 0.3, 0.4]，那么这个样本属于类别 4 的概率就是
$$
p(4) = \frac{e^{0.4}}{e^{0.1} + e^{0.2} +e^{0.3} +e^{0.4}}
$$
对于离散问题，想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。我们只需要关心对正确类别的概率预测，因为只要其值足够大，就可以确保分类结果正确。交叉熵，这个衡量两个概率分布差异的测量函数，就比平方损失函数更适用这种情景。

> TODO reid 常用 Loss，如 softmax loss，triplet loss, contrastive loss, center loss, circle loss

### 分布度量

#### 最大均值差异

（Maximum mean discrepancy，MMD）
$$
MMD(p(a),q(b)) = \\ = \frac{1}{n^2}\sum_{i=1}^{n}\sum_{j=1}^{n}dis(a_i,a_j)  - \frac{2}{nm}\sum_{i=1}^{n}\sum_{j=1}^{n}dis(a_i,b_j) + \frac{1}{m^2}\sum_{i=1}^{m}\sum_{j=1}^{m}dis(b_i,b_j)
$$

#### FID 距离

（Frechet Inception distance, FID）
$$
FID(x, y) = \parallel\mu_x - \mu_y\parallel + Trace(\Sigma_{x} + \Sigma_{y} - 2\sqrt{\Sigma_{x} \Sigma_{y}})
$$
其中，$\mu_x$,$\Sigma_x$分别代表图片 x 的特征向量的均值和协方差矩阵，Trace 表示矩阵的迹，可以理解为对协方差矩阵的特征值求和，这个公式的两项综合考虑了两个矩阵之间的均值和方差

#### KL散度

也就是相对熵，衡量两个概率分布之间的差异。KL 散度度量的不是距离，而是一种信息损失。
$$
D_{KL}(p||q) = \sum_{i=1}^{n}p(x_i)*log\frac{p(x_i)}{q(x_i)}
$$

#### JS 散度

基于 KL 散度的变体，解决了 KL 散度非对称的问题。JS 散度是对称的，其取值在 0 到 1 之间。
$$
JS(p||q) = \frac{1}{2}D_{KL}(p||\frac{p+q}{2}) + \frac{1}{2}D_{KL}(q||\frac{p+q}{2})
$$

> TODO 没有理解这个梯度消失的情况，问问师兄，以及下面的 wasserstein 距离

使用 KL 散度和 JS 散度度量有一个问题，如果两个分布 p, q 离得很远到完全没有重叠的时候，那么 KL 散度值是没有意义的，而 JS 散度值是一个常数。这在深度学习中是比较致命的，这就意味这一点的梯度为 0，梯度消失了。

#### wasserstein 距离

它相对 KL 散度与 JS 散度具有优越的平滑特性，且即使两个分布的支撑集没有重叠，也能反映两个分布的远近，理论上可以解决梯度消失问题，但是其计算量大
$$
W(p, q) =
$$
公式太复杂了，待补充

### 三大相关系数

> pandas 或者 spicy.stats 中有对应实现

反映两个变量之间变化趋势的方向以及程度，其值范围为 -1 到 1。0 表示两个变量不相关，正值表示正相关，负值表示负相关，值越大表示相关性越强。

* 0.8-1.0 极强相关
* 0.6-0.8 强相关
* 0.4-0.6 中等程度相关
* 0.2-0.4 弱相关
* 0.0-0.2 极弱相关或无相关

#### pearson correlation coefficient

定义：用以度量两变量线性相关的程度，它是协方差与标准差的比值。

数据要求：

* 要求数据符合正态分布，在求完皮尔森相关系数之后，通常还会用 t 检验之类的方法来进行皮尔森相关性系数检验，而 t 检验是基于数据处于正态分布的假设。
* 数据之间的差距不能太大，比如：研究人跑步的速度与心脏跳动的相关性，如果人突发心脏病，心跳为0（或者过快与过慢），那这时候我们会测到一个偏离正常值的心跳，如果我们把这个值也放进去进行相关性分析，它的存在会大大干扰计算的结果的。

#### spearman correlation coefficient

定义：记录的是顺序（秩）的相关系数。根据原始数据的排序位置进行求解。

数据要求：无，并且接受异常值，变量值没有变化的情况

#### kendall correlation coefficient

也是一种秩相关系数。

数据要求：计算的对象通常是有序分类变量。如肥胖等级（重度肥胖，中度肥胖、轻度肥胖、不肥胖）。

举个例子，比如评委对选手的评分（优、中、差等），我们想看两个（或者多个）评委对几位选手的评价标准是否一致；或者医院的尿糖化验报告，想检验各个医院对尿糖的化验结果是否一致，这时候就可以使用肯德尔相关性系数进行衡量。

### 无监督行人重识别

#### 基于领域自适应的方法

通过特征的层级来对齐源域和目标域的数据分布，以将源域中的判别性信息迁移到无标记的目标域中。在行人重识别问题中，从特征层级的迁移效果要比从图像层级的迁移效果更好。

![img](https://gitee.com/aboriginalZero/blogimage/raw/master/img/20211005200526.png)

#### 基于伪标记的方法（聚类）

为无标记的数据产生高质量的伪标记来训练和更新神经网络。

未来研究方向：伪标记生成的准确度以及如何有效利用生成的伪标记

<img src="https://img-blog.csdnimg.cn/20210316164156133.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4Mjc2OTcy,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:70%;" />

#### 基于图像生成的方法（GAN）

从图像层面进行风格迁移，其很大程度上依赖于生成对抗网络所生成图像的质量。从监控摄像头获取的行人图像往往质量较低并且存在一些噪声，导致风格转换后图像的质量并不高，因此该类方法在无监督场景下的性能提升并不是很理想。

未来研究方向：需要进一步研究更加适合于行人场景的生成对抗网络来解决该类问题。

#### 基于实例分类的方法

该方法将所有独立的样本当作一个单独的类别来训练网络。该类方法旨在关注如何得到更好的邻近关系 r 来学习模型（如果 x 和 xj 被判定为邻近的样本则 r 为 1，否则为 0）。

未来研究方向：考虑如何采用有效的算法更加精确地进行样本关联度匹配。

### CV 四大任务

分类、定位、检测、分割

#### 图像分类（image classification）

具体要求：为每张图片给上标签

基本思路：以 conv -> bn -> relu -> pool 为基本模型结构，其中卷积层用于提取特征、汇聚层用于减少空间大小。随着网络深度的进行，图像的空间大小将越来越小，而通道数会越来越大。

代表模型：ResNet、DenseNet、SENet

#### 目标定位（object localization）

具体要求：在图像分类的基础上，以 bbox 显示目标在图像中的具体位置

基本思路：多任务学习，网络带有两个输出分支。一个分支用于做图像分类，即全连接+softmax判断目标类别，和单纯图像分类区别在于这里还另外需要一个“背景”类。另一个分支用于判断目标位置，即完成回归任务输出四个数字标记包围盒位置(例如中心点横纵坐标和包围盒长宽)，该分支输出结果只有在分类分支判断不为“背景”时才使用。

#### 目标检测（object detection）

具体要求：目标定位 + 分类。在目标定位中，通常只有一个或固定数目的目标，而目标检测更一般化，其图像中出现的目标种类和数目都不定。

基本思路：1. 基于候选区域 2. 基于直接回归

代表模型：Faster-RCNN、YOLOv5

> 相比于目标检测仅需要把视频中的行人检测和识别出来（只需要做行人和非行人的二分类），目标跟踪还需要识别出分别是什么人（N个行人 + 1个非行人分类）

#### 语义分割（semantic segmentation）

具体要求：目标检测只需要框出每个目标的包围盒，语义分割需要进一步判断图像中哪些像素属于哪个目标。

基本思路：逐像素进行图像分类。将整张图像输入网络，使输出的空间大小和输入一致，通道数等于类别数，分别代表了各空间位置属于各类别的概率，即可以逐像素地进行分类。

#### 实例分割（instance segmentation）

具体要求：目标检测 + 语义分割，需要区分属于相同类别的不同实例。例如，当图像中有多只猫时，语义分割会将两只猫整体的所有像素预测为“猫”这个类别。与此不同的是，实例分割需要区分出哪些像素属于第一只猫、哪些像素属于第二只猫。

基本思路：先用目标检测方法将图像中的不同实例框出，再用语义分割方法在不同包围盒内进行逐像素标记。

代表模型：Mask RCNN，用 FPN 进行目标检测，并通过添加额外分支进行语义分割(额外分割分支和原检测分支不共享参数)，即 Mask RCNN 有三个输出分支(分类、坐标回归、和分割)

### ReID 评价指标

#### Rank-k

值域在 [0,1]，1 是最佳情况。定义 cnt = 0，对于每一个 query，从 gallery 中计算相似度并排序后前 k 个中但凡出现一个匹配的样本 cnt ++，$Rank_k = cnt / size(query)$

#### CMC-k

值域在 [0,1]，1 是最佳情况。目标的正确匹配出现在匹配列表前 k 位的概率。比如 $CMC_5 = 前 5 个候选样本中正确样本个数 / 5$

#### mAP

值域在 [0,1]，1 是最佳情况。

![mAP](https://gitee.com/aboriginalZero/blogimage/raw/master/img/20211005180904.png)

#### mINP

值域在 [0,1]，1 是最佳情况。用于评价一个模型找到最困难样本的能力。比如下图中假设只有 3 个正确样本，list1 在第 10 个才找到最困难样本，而 list2 在第 5 个就找到了。

![image-20211005181649061](https://gitee.com/aboriginalZero/blogimage/raw/master/img/20211005181650.png)

图中只计算一个 query 的 INP，n 个 query 取平均就是 mINP