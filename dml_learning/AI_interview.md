## **AI**

1. **线性回归和逻辑回归区别，推导逻辑回归，梯度下降过程，逻辑回归能用来做回归吗，有哪些缺点。SVM 与 LR（逻辑回归） 的区别，他们的损失函数分别是什么，LR的哪些参数需要调节，LR做多分类的缺点，逻辑回归的全过程（同时问最大似然，损失函数意义和推导，反向传播）**

   **线性回归**：用线性预测函数Y=wX+b来建模，未知的模型参数通过数据来估计。假设数据服从正态分布，通过最小化平方误差的方法，运用最小二乘法来求解参数，来达到预测数据可能值的目的

   **最小二乘法**：基于平方误差最小化来对模型参数求解。（如果X的列数大于行数，X就不是满秩的，XTX的逆求不出来，参数W就无法求解），与梯度下降的区别是通过求导直接得值，没有迭代求值的过程。

   **梯度下降法**：批梯度下降，随机梯度下降，small batch 梯度下降三种。小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果。

   **逻辑回归**：假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。优点：内存资源占用小（只需要存储各个维度的特征值）、适合二分类问题。缺点：不能用于解决非线性问题（决策面是线性的）、难以处理数据不平衡问题。

   **极大使然估计**：根据已有的样本结果，反推出最大概率导致这些样本结果出现的模型参数值（模型已定，参数未知），[梯度下降过程推导](https://blog.csdn.net/csdn_lzw/article/details/79594958)。

   **关于损失函数**：线性回归用的是**平方损失函数**，逻辑回归是平方损失函数加上sigmoid的函数，这是一个非凸的函数，不易求解，用**对数似然函数**得到高阶连续可导凸函数，可以得到最优解。

   **LR如何做多分类**：若所有类别不互斥有交叉的情况，用OvO（N(N-1)个训练器），OvR(N个训练器)，当类别之间有明显互斥，使用softmax分类器

   **LR的哪些参数需要调节**：正则化选择（L2, L1）、优化算法选择（sgd）、分类方式选择（OvO, OvR）

   [SVM与LR的区别](https://blog.csdn.net/keep_giong/article/details/96109592)

2. **svm 什么样的函数能做核函数，为什么要用对偶问题求解？SVM的推导，如何选择核函数，拉格朗日乘数法，KKT条件，SVM求解过程。与感知机的区别。对偶问题，凸二次规划，SMO，多项式核和RBF核的关系？**

   只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用

   KKT条件是更一般的拉格朗日乘子法所需要满足的条件

   训练样本多，维度较大就可以用核函数，如果样本少用核函数比较容易过拟合，特征数等于样本数线性核、特征数少于样本数用高斯核

   对偶问题：把对数据特征（d维）的依赖转变成对数据量（n个）的依赖，并且最后只用到支持向量的数量，减小了计算量。

   凸二次规划：目标函数是二次型函数且约束问题是线性函数（仿射函数）

   SMO：求解二次规划问题的一种方法，其基本思路就是一次迭代只优化两个变量而固定剩余的变量，重复此过程，直到达到某个终止条件程序退出并得到我们需要的优化结果。

   核函数只是用来计算映射到高维空间之后的内积的一种简便方法，是一种任意两个样本之间关系的度量。引入核函数解决样本数据在原始特征空间中线性不可分的情况。

   多项式核：偏线性可分的情境

   RBF核（高斯核）：可以映射到无穷维的特征空间，偏线性不可分的情境

   [svm参见问题](https://www.jianshu.com/p/e26d904f1996)

3. **神经网络理论上可以拟合任意函数的条件。**

   前馈神经网络，只需具备单层隐含层和有限个神经单元，就能以任意精度拟合任意复杂度的函数。（线性+非线性）

4. **哪些算法需要归一化，有哪些常用的归一化手段，为什么**

   我用过的两种，1. 按除最大值。2. 按均值和标准差归一。归一化的作用：第一，加快运算速度（梯度下降时“之”字型）。第二，统一量纲，提高计算精度（避免数值溢出）。需要最优化问题的算法（kmeans，神经网络，LR,SVM）需要进行数据归一化处理，（决策树等不需要的话）。第三、如果使用sigmoid激活函数的话，可以有效避免神经元饱和（输入w为0，sigmoid的输出为0.5）

   常见的有：

   * 均值方差归一化。![X=(x-\mu)/\sigma](https://math.jianshu.com/math?formula=X%3D(x-%5Cmu)%2F%5Csigma)

     这样处理后的数据将符合标准正太分布，常用在一些通过距离得出相似度的聚类算法中，比如 K-means。

   * 最小最大值归一化。![X=(x-Xmin)/(Xmax-Xmin)](https://math.jianshu.com/math?formula=X%3D(x-Xmin)%2F(Xmax-Xmin))

     一种线性的归一化方法，它的特点是不会对数据分布产生影响。不过如果你的数据的最大最小值不是稳定的话，你的结果可能因此变得不稳定。min-max 归一化在图像处理上非常常用，因为大部分的像素值范围是 [0, 255]。

   * 长度归一化。![X=x/\Vert x \Vert](https://math.jianshu.com/math?formula=X%3Dx%2F%5CVert%20x%20%5CVert)

     将特征转为单位向量的形式，可以剔除特征的强度的影响。这种处理用在不考虑向量大小而需要考虑向量方向的问题中，比如在一些文本情感的分类中，我们可能并不需要知道情感表达的强弱，而只要知道情感的类型，比如开心，生气等等

5. **为什么最小二乘法损失是二次，不是其他。**

   在线性回归中，最小二乘法就是试图找到一条直线，使所有样本(数据集)到直线上的欧式距离之和最小。二次指的是损失函数中的预测值与真实值的差值平方。

6. **聚类有哪些算法，k-means的时间复杂度，算法实现**

   * k-means（划分聚类）。速度快，计算简便，但是我们必须提前知道数据有多少类/组。算法步骤：

     1. 随机选取k个中心点;
     2. 遍历所有数据，将每个数据划分到最近的中心点中；
     3. 计算每个聚类的平均值，并作为新的中心点；
     4. 重复2-3，直到这k个中线点不再变化（总距离够小或者达到期望距离）

     时间复杂度O(`I*n*k*m`)，m为每个元素字段个数，n为数据量，I为迭代个数，k为聚类个数；空间复杂度：O(`n*m`)，时空复杂度简化为O(n)

   * DBSCAN（基于密度聚类）。需要确定距离r和minPoints，不需要知道簇的数量，可以发现任意形状（如带状）的聚类，擅长找到离群点（检测任务）。但高维数据有些困难（可以做降维）、Sklearn中效率很慢（数据削减策略）、参数难以选择（参数对结果的影响非常大）。

7. **现代CPU, GPU算力在什么量级**

   常用的NVIDIA GeForce GTX 2080 Ti，拥有24GB显存，一般来说，显存越大，算力也越强

8. **过/欠拟合原因、怎么解决，项目中用过哪些手段，增加网络的的泛化能力**

   **过拟合**：项目中用过数据增强，Dropout，学习率随迭代次数递减，提前终止学习等

   基于数据

   - 数据增强，提高样本的噪声（多样性）
   - 用生成对抗网络增大训练数据量

   基于模型

   - 添加正则化以缩小假设空间，L1，L2正则化
   - 用 Dropout （一定概率使部分神经元不工作）
   - 集成学习
   - 简化模型（降低模型拟合能力）
   - 迁移学习

   **欠拟合**：项目中通过加入交叉特征、增加迭代次数等

   * 增加网络层数
   * 初始更高的学习率

9. **CNN各层说明及其参数量计算**

   计算量决定网络训练快慢；参数量决定所需内存/显存多大。以下是张量在不同层中的改变情况：

   **输入层**

   归一化等数据处理

   **卷积层**

   卷积基本特征：权值共享、稀疏连接（局部感知）；

   意义：提高统计效率、减少参数数量；

   - 权值共享是指在同一个模型的不同模块中使用相同的参数，它是卷积运算的固有属性。使得卷积层具有**平移等变性**。
   - 每个输出神经元仅与前一层**特定局部区域**内的神经元存在连接权重，物理意义是先学习局部特征再组合成全局特征

   

   O=输出图像的尺寸，I=输入图像的尺寸，K=卷积层的核尺寸，N=核数量，S=移动步长，P =填充数。输出图像的通道数等于核数量N。输出图像尺寸的计算公式如下：																				 ![img](https://img2018.cnblogs.com/blog/1093637/201809/1093637-20180925212126418-2257767.png)

   例：AlexNet中输入图像的尺寸为`227*227*3`。第一个卷积层有96个尺寸为`11*11*3`的核。步长为4，填充为0。													  ![img](https://img2018.cnblogs.com/blog/1093637/201809/1093637-20180925212148288-1891687249.png)

   输出的图像为`55*55*96`（每个核对应1个通道）。

   **激活层**

   非线性映射，增加网络表达能力

   激活函数：sigmoid, tanh, ReLU, Leaky ReLU, maxout

   * tanh值域范围是(-1, 1)，调节神经网络的输出（类似于归一化的作用）

   * sigmoid值域范围是(0, 1)，适合用来表示记住多少的信息（遗忘门），因为任何数乘以 0 都得 0，这部分信息就会剔除掉。同样的，任何数乘以 1 都得到它本身，这部分信息就会完美地保存下来。这样网络就能了解哪些数据是需要遗忘，哪些数据是需要保存。

   * ReLU值域范围是（0，+无穷大），相比于sigmoid的优势：避免梯度消失（正半轴导数恒为1）；加速计算（求导为1）；产生稀疏性，缓解过拟合（负半轴梯度为0，参数值不更新）；
   * Leaky Relu：如果用ReLU发现网络欠拟合了，那么试试Leaky ReLU， 这样永远有梯度，参数不会停止更新
   * 自定义激活函数应保证处处可微

   **池化层**

   降采样，特征降维，压缩数据，减小过拟合

   - 均值池化：对背景保留效果好
   - 最大池化：更好的提取纹理信息

   $P_{S}$=池化层尺寸。池化层的输出通道数不改变。输出图像尺寸的计算公式如下：

   ![img](https://img2018.cnblogs.com/blog/1093637/201809/1093637-20180925212235011-259696542.png)

   **全连接层**

   输出向量长度等于（全连接层神经元的数量*1）。

   每层参数量的计算等于权重+偏置，[参考](https://www.cnblogs.com/touch-skyer/p/9150039.html)

10. **优化器有哪些，怎么选择**

    一般来说，无脑选Adam

    - GradientDescentOptimizer (SGD) ：随机梯度下降，批处理数据。
    - MomentumOptimizer：可以使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。
    - AdamOptimizer：综合了AdamGrad 和 Momentum 的优势

11. **如何评价一个分类器，有哪些评价指标，及其适用场景**

    **混淆矩阵**：各种类别的信息一目了然，可以快速找到分类错误的类别。

    **F1指数**：精准率和召回率的调和平均值，如果希望F1值高，则需要两者都比较高且均衡。**PR曲线**就是横坐标召回率，纵坐标精准率，取先与y=x的交点就是平衡点。

    **AUC**：AUC被定义为**ROC曲线**下的面积，反映出分类器对样本的排序能力，AUC越接近1，能将正负样本分的越开。ROC横坐标是预测为正但实际为负的样本占所有负例样本的比例，ROC纵坐标：预测为正且实际为正的样本占所有正例样本的比例，ROC曲线较之于Precision，Recall最大的好处就是**对数据不平衡不敏感**。

    **适用场景**：在信息检索、推荐中优先考虑Precision；在安全检测中优先考虑Recall；PR曲线适用于评估更看重正例的情况；ROC适合有多份数据且存在不同类别分布，评估分类器整体性能的情况。

12. **batch norm的原理，具体训练测试是怎么做的，dropout的原理，layer norm的原理**

    每次训练、测试数据存在分布不一致的问题，批量规范化可以突出绝对差异，减小相对差异。在训练时，一般把batch norm加在卷积层之后，激活函数之前，首先计算批量的均值和方差，然后归一化，最后缩放和平移（缩放因子和平移因子是可训练的）；测试因为只有一张图片，拿训练中得到的均值和方差来计算就行。

    有了批量规范化之后，可以使后层网络面对稳定的输入值，降低梯度发散的可能，从而加速了训练 ，并且泛化效果会更好，且其也有部分正则的作用，可以替代dropout。

    加入缩放平移变量的原因是：保证每一次数据经过归一化后还保留原有学习来的特征，同时又能完成归一化操作，加速训练。 这两个参数是用来学习的参数。

    卷积层之前的BN慢慢替代了全连接层的Dropout。

    dropout：在网络训练阶段的每一次迭代，选择部分神经元让其暂时不参与前向推理和后向传播，所有子模型参数共享。能发挥其作用的地方在全连接层，可当代的深度网络中，全连接层也在慢慢被全局平均池化层所取代，不但能减低模型尺寸，还可以提升性能。

    BN 和LN的区别：

    - LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；
- BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。
    - LN适用于RNN，BN适用于CNN。因为RNN的深度是不固定的，不同的time-step需要保存不同的statics特征，可能存在一个特殊sequence比其他sequence长很多，这样training时，计算很麻烦
    
    [参考](https://blog.csdn.net/wzy_zju/article/details/81262453)

13. **特征工程怎么做的、如何判断特征的有效性**

    特征工程就是选择一些表示典型特征的数据，来替代原始数据作为模型的输入，进而得到比较好的输出效果。

    - 避免无意义、重复性、复杂的信息
    - 找具有物理意义的特征或有统计意义
    - 一般来说对语音数据做傅里叶变化；图片数据拿颜色、轮廓；语句做embedding

    如何判断：使用较多的一类标签来作为预测值，准确率为最多的一类标签/整体数目，至少要比这个值好；找一个baseline模型；

14. **LeNet、vgg、resnet等经典网络的发展**

    - LeNet - 5 （1998）：引入「卷积池化卷积池化」的一般网络结构
    - AlexNet （2012） ：引入 Relu，Dropout，多GPU训练，对图片的随机裁剪以达到数据增强
    - VGG（2014）：卷积层使用更小的 filter size 和 strides ，如`1*1`和`3*3`
      - 多个`3×3`的卷基层比一个大尺寸 filter 卷积层有更多的非线性，使得判决函数更加具有判决性
      - 多个`3×3`的卷积层比一个大尺寸的 filter 有更少的参数。假设卷基层的输入和输出的特征图大小相同为C，那么三个3×3的卷积层参数个数3×（3×3×C×C）=27CC；一个7×7的卷积层参数为49CC；所以可以把三个3×3的filter看成是一个7×7filter的分解（中间层有非线性的分解）
      - `1*1`的卷积核的作用是通过减少通道数量，来减少卷积核参数，不影响输入输出维数，1*1卷积核的通道数a, 下一层卷积核的通道数为b, 长宽为w,h，那么下一层的参数为 `w * h *a * b`。
    - GoogLeNet（2014）：引入 Inception 结构，中间层的辅助 LOSS 单元，后面的全连接层替换成简单的全局平均 pooling 。
      - Inception 结构中引入了 `1*1`卷积核实现了降维，减少计算次数。
      - 网络中 3 个 LOSS 单元是为了帮助网络的收敛。中间层的辅助 LOSS 单元，目的是计算损失时让低层的特征也有很好的区分能力，从而让网络更好地被训练。
      - 引入简单的全局平均 pooling 会让参数变得更少。全连接层很影响网络参数个数。
    - ResNet（2015）：网络层数超百层，引入残差单元来解决退化问题。
    - DenseNet （2017）：密集连接，但是很占用内存！
      - 缓解梯度消失问题
      - 加强特征传播，鼓励特征复用，极大的减少了参数量

15. **L1、L2正则讲一下，区别，以及各自的使用场景**

    正则化：限制参数的取值范围，从而防止过拟合。

    L1正则化：在损失函数中加入权值向量w的绝对值之和。权重稀疏。

    L2正则化：在损失函数中加入权值向量w的平方和。权重平滑。

    区别通过作图，等高线是目标函数的可能取值，横纵坐标是俩参数

    一般来说选L2正则化，对于需要对特征进行筛选的场景，那我们可以选择L1正则

16. **知识图谱关系抽取的技术发展**

    不考虑远程监督，因为主动学习采取的是不同于远程监督的一种减少监督信息的方法

    CNN（2014）:模型首先通过预训练或者随机初始化的 embedding 将句子中的词转化为词向量，同时使用句子中的实体词及其上下文相对位置表征实体词的位置向量；随后通过 CNN 网络抽取句子级别的特征（词向量+位置向量），并使用池化方式得到压缩后的特征向量表示，最后将特征向量输入一个全连接的神经网络层对句子所表述的关系进行分类。

     BiLSTM+ATT（2016）：考虑到CNN难以建模句子中长距离的依赖关系，句子中每个词的重要程度不一，此模型通过双向LSTM网络抽取句子的特征向量，并且加入注意力机制d对输出的特征向量赋予不同的权重，最终生成有偏向性的向量表示，输入全连接神经网络层最终实现对关系的分类。

    BERT（2018）：双向Transformer的Encoder，模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。

17. **距离和相似性度量方法**

    一个距离函数，要满足**自达性、非负性、对称性、三角形法则**。

    假设数值点 P 和 Q 坐标如下：

    ![img](https://images0.cnblogs.com/blog/533521/201308/07220422-b6c5a38eccb74824b92ba1b40c9dd92f.png)

    那么，明式距离定义为：
    ![img](https://images0.cnblogs.com/blog/533521/201308/07220504-12655edb08dc45ae8a036d8028743042.png)

    当p为1、2时，分别对应**曼哈顿距离**（除绿线）、**欧式距离**（绿线），越小越相似

    ![img](https://images0.cnblogs.com/blog/533521/201308/07220530-1c87c470c5984305932cb5f5fc91656f.png)

    当p取无穷大，对应**切比雪夫距离**，即多维平面下各坐标数值差绝对值的最大值

    ![img](https://images0.cnblogs.com/blog/533521/201308/07220549-4fb4c30e7fb84ca290d04f44f75dea7b.png)

    当p取不同值时，对应到原点距离为：

    ![img](https://images0.cnblogs.com/blog/533521/201308/07220559-ae662025d1394f90bfd62f7c21c3d895.png)

    明式距离与数据的分布无关，如果 x 方向的幅值远远大于 y 方向的值，这个距离公式就会过度放大 x 维度的作用，所以在计算距离之前，我们要先做归一化 。

    但当数据各个维度相关（例如：身高较高的信息很有可能会带来体重较重的信息，因为两者是有关联的），需要用到**马氏距离**，（下图，绿黑马氏距离小于红黑）跟PCA有点点像。

    ![img](https://images0.cnblogs.com/blog/533521/201308/07220637-f472bb13a779481bbfa45a9d79bd2175.png)

    **余弦距离**，与向量的幅值无关，只与向量的方向相关

    ![img](http://latex.codecogs.com/gif.latex?CosSim(x,y)%20=%20\frac{\sum_i%20x_i%20y_i}{%20\sqrt{%20\sum_i%20x_i^2}%20\sqrt{%20\sum_i%20y_i^2%20}%20}%20=%20\frac{%20\langle%20x,y%20\rangle%20}{%20||x||\%20||y||%20})

    但其受到向量的平移影响，即如果将 x 平移到 x+1, 余弦值就会改变。于是，引入**皮尔逊相关系数**，越大越相关。

    ![img](http://latex.codecogs.com/gif.latex?\begin{align}%20Corr(x,y)%20&=%20\frac{%20\sum_i%20(x_i-\bar{x})%20(y_i-\bar{y})%20}{%20\sqrt{\sum%20(x_i-\bar{x})^2}%20\sqrt{%20\sum%20(y_i-\bar{y})^2%20}%20}%20&=%20\frac{\langle%20x-\bar{x},\%20y-\bar{y}%20\rangle}{%20||x-\bar{x}||\%20||y-\bar{y}||}%20%20%20&=%20CosSim(x-\bar{x},%20y-\bar{y})%20\end{align})

    相关系数具有平移不变性和尺度不变性，计算出了两个向量（维度）的相关性。例如，在推荐系统根据为某一用户查找喜好相似的用户，进而提供推荐，优点是可以不受每个用户评分标准不同和观看影片数量不一样的影响。

    **汉明距离**：两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。

    **编辑距离**：转换成另一个不同长度的字符串所需要的最小替换次数。越小越相似。

    **杰卡德相似系数**：两个集合A和B交集元素的个数在A、B并集中所占的比例，值越大说明A、B相似度越高，常用于推荐系统。

    ![clip_image013](https://images0.cnblogs.com/blog/407700/201306/28144621-9d519167c5b54643ba662ea8d10e3d33.gif)

18. **短文本/句子相似度如何计算**

    * 无监督

      - 词袋模型（BOW）

        建立映射表，不考虑词法、语序问题。

      - TF-IDF模型

        一个词的权重由TF（词频） * IDF（逆文档频率） 表示，一个词在这篇文本中出现的频率乘上一个词在所有文本中出现的频率倒数取对数。假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是3/100=0.03。如果“母牛”一词在1,000份文件出现过，而文件总数是100,000份的话，其逆向文件频率就是 lg(100,000 / 1,000)=2。最后的TF-IDF的分数为0.03 * 2=0.06。经常用于长文本或文章的相似度计算。

      - 构造词向量

        使用大规模语料库通过`word2vec`训练出词向量，对短文本分词，把所有词对应的词向量进行求和（以`SIF`，`TF-IDF`等为权重或直接取平均），获得句子向量。对两个短文本句子向量进行距离度量，获得其相似度值。

    * 有监督

      * Siamese

        将两个短文本分别输入到相同的深度学习编码器中（CNN、RNN），使得两个句子映射到相同的空间中，然后将得到得两个句子向量进行距离度量，最终获取短文本的相似度值。

        优点：权值共享、容易训练。

        缺点：在映射过程中两个文本之间没有明确的交互作用，会丢失很多相互影响的信息。

      * ESIM

        在经过编码器得到句子向量后，通过注意力机制将两个句子向量进行信息的交互，最终将其聚合成一个向量，并通过值映射（全连接到一个节点）获取短文本的相似度值。

      * BERT

        它主要采用两阶段模式，第一阶段使用很大的通用语料库训练一个语言模型，第二阶段使用预训练的语言模型做相似度计算任务，即将两个文本输入到预训练模型中，得到信息交互后的向量，并通过值映射（全连接到一个节点）获取短文本的相似度值。

    * 有监督+无监督

      无监督学习的弊端在于过词向量人为地加权求和并不能得到很好的能够表征句子且包含上下文语义信息的句向量。所以通过有监督的方式，如孪生网络去获取一个短文本的句向量，虽然没有两个文本句向量直接的相互信息，但是包含了各自文本的上下文语义信息。

      为了避免每一次来一个新的文本，都将所有文本都计算一遍。我们舍弃文本之间的交互作用，直接使用生成好的句向量。当新文本来时，只需要求解新文本的句向量，然后与存储在库中的句向量进行距离度量，就可以获得最相似的文本了。

      目前代表模型是：Sentence-BERT，其将BERT模型代替原来孪生网络中的CNN或LSTM结构，获取具有更多语义信息的句向量，将监督模型在工业界使用变成了可能。bert-as-service用于实际工程，支持高负载获取词向量、句向量等embedding。

19. **EM是什么，举个例子**

    EM算法根据已有的样本结果，反推出最大概率导致这些样本结果出现的模型参数值，模型未知，参数未知。模型未知，所以我们引入一个隐变量。

    初始版EM：随机初始化模型参数，用它极大似然估计隐变量z（z等于某一个具体值），然后隐变量z，极大似然估计模型参数，不断迭代。

    进阶版EM：随机初始化模型参数，考虑所有可能的z值（拿到z的概率分布），对每一个z值按照概率大小加权相加，然后再去极大似然估计模型参数，不断迭代。

    [扔硬币的例子](https://www.jianshu.com/p/1121509ac1dc)

20. **解释下信息熵、交叉熵、最大熵**

    **信息量**：与信息发生的概率成反比
    $$
    I(x) = -\sum_{i=1}^n log(P(x_i))
    $$
    **信息熵**：也叫熵，是随机变量不确定性的度量，不确定性越大，熵值越大；若随机变量退化成定值，熵为0（概率总是在0-1之间，log后会小于0，取负号以后熵就是正数）
    $$
    H(p) = -\sum_{i=1}^np(x_i)*log(p(x_i))   \\ = \sum_{i=1}^np(x_i)*log\frac{1}{p(x_i)}
    $$
    **最大熵**：系统中事件发生的概率满足一切已知约束条件，不对任何未知信息做假设，也就是对于未知的，当作等概率处理。

    **相对熵（KL散度）**：衡量两个概率分布之间的差异。
    $$
    D_{KL}(p||q) = \sum_{i=1}^{n}p(x_i)*log\frac{p(x_i)}{q(x_i)}
    $$
    在机器学习中，常使用 p(x) 来代表样本的真实分布，q(x) 来表示模型所预测的分布，比如在一个三分类任务中（猫马狗分类），x1, x2, x3分别代表猫、马、狗，例如，一张猫的图片的真实分布p(x) = [1, 0, 0]，预测分布为 q(x) = [0.7, 0.2, 0.1]，计算KL散度：
    $$
    D_{KL}(p||q) = \sum_{i=1}^{n}p(x_i)*log\frac{p(x_i)}{q(x_i)}\\= 1 * log\frac{1}{0.7} + 0 * log\frac{0}{0.2} + 0 * log\frac{0}{0.2} = 0.36
    $$
    KL散度越小，表示P(x)与Q(x)的分布更加接近，可以通过反复训练Q(x)来使Q(x)的分布逼近P(x)。

    **交叉熵**：等于KL散度+信息熵（在具体ML任务中是常量），常用来作为损失函数。
    $$
    H(p,q) = -\sum_{i=1}^np(x_i)*log(q(x_i)) \\ = \sum_{i=1}^np(x_i)*log\frac{1}{q(x_i)}
    $$
    在机器学习训练网络时，输入数据与标签常常已经确定，那么真实概率分布P(x)也就确定下来了，所以信息熵在这里就是一个常量。由于KL散度的值表示真实概率分布P(x)与预测概率分布Q(x)之间的差异，值越小表示预测的结果越好，所以需要最小化KL散度，而交叉熵等于KL散度加上一个常量（信息熵），且公式相比 KL 散度更加容易计算，所以在机器学习中常常使用交叉熵损失函数来计算loss。

    **交叉熵与softmax函数**：softmax函数将输出值变成值为正且和为1的概率分布，softmax回归适用于分类（离散）问题。

    softmax回归跟线性回归一样将输入特征与权重做线性叠加，不同在于softmax回归的输出值个数等于标签里的类别数（线性回归是一个），更适合离散值的预测和训练。

    softmax公式：这里假设一个样本，它有 n 种类别，在进入 softmax 之前有一层全连接层，它的输出是 $ a_1, a_2,...,a_n$，那么这个样本属于类别 i 的概率为：
    $$
    p(i) = \frac{e^{a_i}}{\sum_{j=1}^{n}e^{a_j}}, \forall i \in 1...n
    $$
    上式可以保证属于各类别的概率和为1。举个例子，如果全连接层的输出是 [0.1, 0.2, 0.3, 0.4]，那么这个样本属于类别 4 的概率就是
    $$
    p(4) = \frac{e^{0.4}}{e^{0.1} + e^{0.2} +e^{0.3} +e^{0.4}}
    $$
    

    对于离散问题，想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。我们只需要关心对正确类别的概率预测，因为只要其值足够大，就可以确保分类结果正确。交叉熵，这个衡量两个概率分布差异的测量函数，就比平方损失函数更适用这种情景。

21. **梯度消失/爆炸问题 原因，解决措施。**

    |                    | 梯度爆炸                                           | 梯度消失                                                 |
    | ------------------ | -------------------------------------------------- | -------------------------------------------------------- |
    | 原因               | 1. 网络层数过多 2.权值初始化过大或学习率过大       | 1. 网络层数过多 2. 采用了不合适的损失函数（比如sigmoid） |
    | 方法               | 1.权重正则化                                       | 1.resnet结构 2.LSTM结构                                  |
    | 适合两种情况的方法 | 1.选用Relu等激活函数 2. 批量规范化 3. 学习率的设定 | 1.选用Relu等激活函数 2. 批量规范化 3. 学习率的设定       |

    梯度爆炸/消失问题其根本原因在于反向传播时权重的过大或者过小。

22. **深度学习是否需要交叉验证。**

    数据量足够的情况下，一般不需要，因为K折交叉就需要训练K个神经网络。如果想要提高泛化性，可以考虑用dropout，能起到交叉验证的效果 。

23. **每个项目负责了哪些东西**

    * 手势识别
      * 我的工作：搭建CNN,LSTM网络
      * 特征工程：将一段基带信号转换为同向和正交信息；采用8个频率的信号；每0.1秒能有110个采样点，最终是`2*8*550`
      * 网络设计：由于特征简单易分，CNN网络结构接近于Mnist网络，3卷积池化+2全连接；因为手势信号的不定长性和时序性，所以引入LSTM，把CNN提取到的若干段微手势特征作为时序信号顺序输入LSTM模型
      * 意义：智能家居中减少对遥控器、移动终端的依赖；车联网中通过手势唤醒车载系统的娱乐功能；
      * 不足及解决：特征工程做的不够，能识别的手势还比较少；信号降噪问题，受静态变量影响（比如没有经过反射直接从扬声器传入麦克风的信号分量）
    * 卡号识别
      * 我的工作：作为队长，基本每个模块都参与
      * 特征工程：数据增强，照片倾斜、强弱光环境、卡号模糊等实际可能存在的客观因素
      * 模型搭建：参加开源项目，根据卡号识别情况做适配
      * 相较于其他队伍的优势：考虑实际情况做数据增强、加入银行卡号先验知识、引入不同类间的非极大值抑制

24. **比赛中如何解决样本不平衡，以及用了什么训练技巧，为什么能起作用，其原理是什么**

    样本不平衡解决方法：扩大数据集（人工收集或者数据增强）；尝试其他评价指标；在损失函数上赋予大小类不同的权重

    对小类数据过采样，银行卡任务中，通过对特殊样本如卡号与背景颜色一样的样本通过模糊度调节、高亮叠加等数据增加（数据增强）操作来缓解样本不平衡问题。

    **训练技巧**

    输入尺寸很重要，很影响模型性能

    数据增强要往实际可能发生的情况增强，在卡号识别情境下就是照片倾斜、强弱光环境、卡号模糊等客观因素，因为最终还是为了丰富实际样本分布的信息量。

    银行卡号的前六位代表卡片属性和发卡行，具有特殊含义，数字的组合方式是有限的，基于这一先验知识，我们制作了银行卡号与银行卡归属地的字典。通过字符串匹配（编辑距离算法）实现自动纠错功能

    通过实验发现，在预测数字框时，可能会出现单数字多预测框的情况。在原有NMS的基础上增加不同类间的非极大值抑制。

25. **YOLO系列的理解，one stage，two stage的区别**

    **YOLOv1**

    * 提出one-stage方法，将整张图片作为输入，直接在输出层回归 bounding box 的位置及其所属类别。

    **YOLOv2**

    * 使用BN。对每一个卷积层增加BN，提高网络收敛性
    * 去掉全连接，使用anchor box来预测bounding box。通过预测anchor box偏移值与置信度，而不直接预测坐标值。
    * 多尺度训练。网络每迭代几次会改变网络参数，每10个batch，网络随机选择新图片尺寸

    **YOLOv3**

    * 多尺度预测。从单层预测五种bounding box变成每层3种bounding box。
    * 使用逻辑回归代替softmax分类，Loss用二分类交叉熵替代原来的均方误差。因为softmax使得每个框只能识别一个类别，无法解决标签中有“人“，”小孩“这样的同时预测。
    * 替换backbone。采用Darknet-53，加了残差网络，大大提高了网络提取特征的能力。

    单阶段检测模型与两阶段的区别在于没有中间检测区域再输入给CNN的过程，直接回归便完成了对位置和类别的判定。

    [参考](https://blog.csdn.net/yes_doit/article/details/88723102)

26. **如何快速查找相似向量(百万级、千万级)，说一下KD树，KD的时间复杂度，KD最坏情况下应该怎么办**

    KD树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形结构。构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间划分，构成一列的k维超矩形区域。

    如果实例点是随机分布的，则kd树搜索的平均计算复杂度是O(logN)。kd树更适用于训练实例数远大于空间维数的情况，如果训练实例数接近空间维数，则效率退化为线性扫描。

27. **RNN/LSTM/GRU的区别，RNN的缺陷以及LSTM怎么解决？RNN为什么出现梯度消失，LSTM的公式**

    RNN：当时间步数过大/小时，梯度容易出现爆炸/衰减

    LSTM：引入门控机制解决梯度问题，可以从语料中学到长期依赖

    GRU：LSTM中的计算参数是RNN的4倍，通过去掉细胞状态，使用隐藏状态进行信息的传递，参数运算更少，所以相比于LSTM训练更快一些

    RNN梯度消失/爆炸以及三种结构的公式推导：[BPTT算法](https://zhuanlan.zhihu.com/p/60915302)

    [三种结构动图演示](https://blog.csdn.net/dqcfkyqdxym3f8rb0/article/details/82922386)

28. **加速训练的方法**

    在特征学习不显著下降的时候，减少网络层数；自适应学习率；提前终止；在显存不爆的前提下，batch_size越大越好（提高内存利用率，减小训练震荡）；GPU加速等

29. **深度学习和机器学习的区别**

    - 所需数据量
    - 执行时间
    - 分治求解、端到端
    - 深度学习里面特征提取器比较简单

30. **BP是怎么求导的，正向反向手推**

    感知机：由两层神经元组成（输入层、输出层），无法处理线性不可分问题（异或问题）

    多层感知机：含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换

    前向传播统计误差，反向传播更新权重

31. **用YOLO速度、性能如何做的优化，比赛中采用了哪种手段，效果为何有提升？**

    * 增大网络输入层的尺寸，提高图片的分辨率，提高精度
    * 在预测数字框时，可能会出现单数字多预测框的情况。在原有NMS的基础上增加了不同类间的非极大值抑制
    * 通过透视、背景替换、高亮叠加、模糊度调节等手段进行数据增强，尽可能考虑实际情况中的用户习惯
    * 基于先验知识的自动纠错。银行卡号的前六位代表卡片属性和发卡行，具有特殊含义
    * 客户端图片压缩，减少图片传输时间

32. **求两个旋转矩阵的IOU（YOLO里面有涉及到）**

    ```
    def compute_iou(rec1, rec2):
        '''
        计算交并比 IOU
        :param rec1:  (top, left, bottom, right) 元组
        :param rec2:  (top, left, bottom, right) 元组
        :return: IOU的值
        '''
        S_rec1 = (rec1[2] - rec1[0]) * (rec1[3] - rec1[1])
        S_rec2 = (rec2[2] - rec2[0]) * (rec2[3] - rec2[1])
    
        sum_area = S_rec1 + S_rec2
    
        left_line = max(rec1[1], rec2[1])
        right_line = min(rec1[3], rec2[3])
        top_line = max(rec1[0], rec2[0])
        bottom_line = min(rec1[2], rec2[2])
    
        if left_line >= right_line or top_line >= bottom_line:
            return 0
        else:
            intersect = (right_line - left_line) * (bottom_line - top_line)
            return intersect / (sum_area - intersect)
    ```

33. **深度学习调参经验**

    利用tensorboard绘制损失函数曲线、训练/测试准确率曲线

    1. 测试准确率先升后降，可能是学习率太大了。
    2. 对于整个网络的输入输出，都要做归一化处理。
    3. 如果想提前终止学习，那么在修改迭代次数的同时，也要降低学习率。
    4. 池化层中的 stride > kernel 时，容易产生 NaN 。
    5. 通过分别观察各层的 loss 、diff 可以判断是否是一个逐渐收敛的过程。
    6. 5000个以下的样本算是样本容量不足
    7. top 命令监视CPU、nvidia -sml 监视GPU
    8. 通过Cprofile来看到程序运行时间分配、通过tracemalloc来查看程序运行内存分配
    9. 采用 BN 之后，可以移除 Drop-out , L2 正则项参数，初始学习率也可以较大，用在激活层前后。
    10. 对于新构建一个神经网络，先优化参数，再优化结果，交替进行直至得到满意的模型。

34. **如何进行实体消歧，如“普通老百姓”，可能是电视剧名字，也可能是人的称谓**

    构造一个含有实体描述句子（或关键字）的实体库，对于目标句子，判断其与每一个可能的实体概念描述句子之间的相似度。

35. **词向量的方法**

    1. 独热表示。
       * 基于one-hot、tf-idf、textrank等的bag-of-words，存在维度灾难、语义鸿沟问题

    - 分布式表示。

      基于分布式假设，即相同上下文语境的词有相似含义

      - 矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大

      - 基于词向量的固定表征

        优化效率高，但是基于局部语料

        - word2vec：基于滑窗、无监督、交叉熵、hierarchical softmax、negative sampling
        - fastText：可以做有（无）监督、subword，N-gram

        基于全局预料，结合了LSA和word2vec的优点

        - glove：看成对LSA矩阵分解的优化、Adagrad、均方误差

      - 基于词向量的动态表征。可以解决一词多义的问题

        - elmo：LSTM进行提取、双向语言模型
        - GPT：Transformer-decoder提取、单向语言模型
        - bert：Transformer-encoder提取、双向语言模型

39. **深度学习是万能的吗？什么地方不适用，如果给你一个任务，你如何选择用深度学习还是传统的如SVM？（任务、数据量、数据特点），按照你的经验，深度学习的待学习的参数量和训练样本数量之间的关系。**

    看特征，比如都是离散值，那决策树就比较合适；如果是图片，那肯定要cnn。数据量多的时候，用深度的方法，深度学习的 learning capacity 比传统的方法要大，传统方法在数据量很多的时候，性能几乎就不增长了，比如svm，只跟支持向量有关，数据量再多也没用，

    其实除了图片和自然语言的数据，深度学习不怎么用的上，深度学习主要的优势是可以自动地提取特征，也就是representation learning。

    DL的带学习参数量跟模型结构有关，正相关。

40. **1*1的卷积核的作用**

    2014年的GoogleNet中提出的。

    * 改变模型维度。特征升维还是降维取决于卷积核的个数
    * 减少模型参数。代替全连接层的使用或是加在两个卷积层之间
      * 假设输入是224x224x3 的图像，假设经过特征提取之后是7x7x512的，那么传统的方法应该将其展平成为1*(7x7x512)，然后做全连接层，假设全连接层为4096×1000层的（假设有1000个分类结果）。 那么用1×1卷积核怎么做呢，先选择4096个7×7×512的卷积核做卷积，这样得到一个[1×1×4096]的特征图，然后再选择用1000个1×1×4096的卷积核，这样就能得到一个[1×1×1000]的特征图。
    * 增加模型非线性表达能力。在保证feature map尺度不变的基础上，利用后接的非线性激活函数增加

41. **TensorFlow Interactivesession 和 Session的区别**

    构造交互环境下的会话，方便使用`tensor.eval()`，实时查看张量的值

42. **TensorFlow、PyTorch内部求导机制**

    神经网络的前向传播定义了一个计算图，图中的节点是 Tensors，边表示从输入张量产生输出张量的函数。使用自动微分来计算神经网络的反向传播。

    * TensorFlow是静态图，只定义一次，然后输入不同的数据，重复执行
    * Pytorch是动态图，可以对数据点执行不同的计算，例如，对于每个数据点，循环网络可以展开不同数量的时间步长

    Pytorch中的`autograd`包配合`Variable`对象，每个原始`autograd`操作符实际上是在`Tensors`上操作的两个函数。`forward` 函数从输入张量来计算输出张量，`backward`函数接收输出向量关于某个标量的梯度，然后计算输入张量关于同一个标量的梯度。

    [PyTorch和TensorFlow的简单代码实例](https://www.jianshu.com/p/52684285e335)

44. **常见的分类算法有哪些**

    SVM，KNN，决策树，DNN，

45. **CNN的原理说一下**

    从输入层、卷积层、激活层、池化层、全连接层来展开说明

46. **为什么梯度是函数变化最快的方向，梯度下降是否一定是全局最优点**

47. **极大似然的原理**

    根据已有样本结果，反推出最大结果导致这些样本结果出现的模型参数值

49. **Seq2Seq，Attention机制,手写代码**

    经典的翻译例子法语到英语的翻译，肯定不能要求输入输出一样长，于是引入encoder-decoder机制，由encoder编码到语义空间和decoder根据语义空间解码翻译成一个个的英语句子。

    但是，仅用一个固定长度的context向量来编码所有的语义，是很困难的，所以引入attention机制，attention通过对所有token做内积（余弦距离），得到的值越大，说明越重要，分配的权重也就越高。

51. **数据预处理方法，特征工程怎么做的，为什么选这些特征，为什么归一化，类别数据怎么处理，id型数据怎么处理，数据不均衡怎么办等。数据增广方法**

    最好选择有实际意义的，或分解的特征

    离散特征二值化（比如今天周几，是周一或者不是周一，是周二或者不是周二）

    连续值特征按区间划分，等量划分或等值划分

    缺省值可以单独表示或众数或平均数

    ID型数据通过特征嵌入embedding的方法获取稠密向量，随机初始化一个特征嵌入矩阵，让网络反向传播来优化。

    地理特征可以处理成经纬度的连续特征，也可以分成按省份、城市的离散特征

52. **shuffle阶段的过程详细介绍一下**

    洗牌算法

53. **用过哪些可视化组件，如何对数据进行可视化分析**

    tensorboard，根据迭代次数查看Loss，评价指标

54. **命名实体识别与关系抽取的任务是在干什么？目前常用的模型是哪些？自己项目中怎么用的**

    拿到一个句子中的实体，以及实体之间的关系，对于关系抽取，目前有联合抽取和单独抽取，我只做过单独抽取的，通过R-BERT来抽取关系，

55. **阐述BiLSTM的BP过程，为何BiLSTM后接一层CRF会有提升？CRF层自己是怎么实现的。CRF的原理是什么**

    实体抽取的问题，搁置

56. **Python GC机制的原理，Python对象在内存中的存储差异。**

    对于可变数据类型（List ，Dict），支持内存中相同值的对象有多个,而不可变数据类型（String，常量，元组），内存中只有一个相同值的对象。

    [参考](https://blog.csdn.net/as480133937/article/details/87305247)

57. **resnet、inception，attention分别描述**

58. **自己实现过哪些机器学习算法？单机实现还是多线程实现的**

    Kmeans算法、单机

62. **现阶段比较成熟的NER模型有哪些？相关paper是否读过？有没有自己复现模型？可以改进的地方在哪里**

63. **进行RE时的难点在哪里？通识领域与特定领域进行RE时的差别在哪里？**

    关系表达的多样性（一种关系类别有多种表达）、隐含性（没有明显的指定）、时空性（前期、前总统等）、多关系（一个句子中有多种关系）

    需要增加额外的字典

64. **embedding怎么做的**

    Word2vec 的CBOW和Skip-gram

65. **jieba分词原理**

    动规、维特比算法等

66. **max pooling 梯度传导**

    经过池化层前后的梯度保持不变，比如最大池化直接把整个梯度传递给上一个的某个元素；

    均值池化是把某个元素的梯度等分为n份。这样保证池化前后的梯度之和不变。

67. **crf+bilstm序列标注的细节**

68. **GPU资源利用率低怎么办**

    * 显存占用率低：增加模型层数来改变模型；增大batch_size

    * GPU利用率低（Volatile GPU-Util）：I/O问题导致的，改善CPU或者加内存条；或者在编码时用DataLoader中设置num_workers，pin_memory等参数

69. **elmo BERT大致的介绍，bert（position encoding为什么选三角函数？bert强于rnn的地方？我回答并行，接着问我为什么并行。transformer使用的时候，制约显存的最关键因素是什么？）**

70. **你觉得nlp将来可以做成什么落地的产品**

    语音同传技术、改善搜索引擎技术、网上舆论观点抽取帮助产品宣传

71. **你对自己的定位是什么**

    想通过技术观察社会百态

https://www.nowcoder.com/discuss/112562?type=0&order=0&pos=18&page=1