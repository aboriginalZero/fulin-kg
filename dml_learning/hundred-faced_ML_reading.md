## 一、特征工程

1. 为什么需要对数值类型的特征做归一化？  

   同一量纲，使得不同指标之间具有可比性；加快模型梯度下降

2. 在对数据进行预处理时， 应该怎样处理类别型特征？  

   * 序号编码：类别间具有大小关系（比如成绩高中低）

   * 独热编码：类别较少且类别间不具有大小关系

   * 二进制编码：适合有交叉的特征

3. 什么是组合特征？ 如何处理高维组合特征？  

   把一位离散特征两两组合构成高维组合特征

   用低维向量表示，比如embedding

4. 怎样有效地找到组合特征？

   待填

5. 有哪些文本表示模型？ 它们各有什么优缺点？  

   * 词袋模型与N-gram模型

     每个词是一个单独的特征向量，用TF-IDF来计算权重（一个词在这篇文本中出现的频率乘上一个词在所有文本中出现的频率倒数取对数）

     N-gram模型：由于单词级别的划分往往丢失过多语义信息，所以可以按每N个单词来组成一个单独的特征向量

   * 主题模型

     待填

   * 词嵌入与深度学习模型

     把每一个词映射为低维空间上的稠密向量，但这样的表示过于底层，所以需要配合深度学习模型使用。

6. word2vec是如何工作的，与LDA的区别

   word2vec是一种浅层的神经网络模型，包括输入层、隐藏层、输出层，softmax层。有两种网络结构：CBOW和Skip-gram。

   损失函数就是让语料库中所有单词的整体生成概率最大化，最终使用的是输入层到隐藏层的权重矩阵作为词的稀疏向量表示。

   由于softmax每次计算都要遍历整个词表，所以引入层次softmax和负采样。

   层次softmax通过构造哈夫曼树，将多分类转成多个二分类，复杂度为log级别。

   负采样通过划分正例负例，引入新的损失函数，使正例生成概率更大、负例生成概率更小。

   与LDA的区别 待填

7. 在图像分类任务中， 训练数据不足会带来什么问题？ 如何缓解数据量不足带来的问题？  

   训练数据不足可能导致欠拟合/过拟合。

   过拟合：项目中用过数据增强，Dropout，学习率随迭代次数递减，提前终止学习等

   基于数据

   - 数据增强，提高样本的噪声（多样性）
   - 用生成对抗网络增大训练数据量

   基于模型

   - 添加正则化以缩小假设空间，L1，L2正则化
   - 用 Dropout （一定概率使部分神经元不工作）
   - 集成学习
   - 简化模型（降低模型拟合能力）
   - 迁移学习

   欠拟合：项目中通过加入交叉特征、增加迭代次数等

   * 增加网络层数
   * 初始更高的学习率

## 二、模型评估

1. 准确率的局限性

   无法应对数据不平衡的情况（1个正类99个负类，全部预测为负类，准确率为99%）

   可以通过计算每个类别的准确率求平均来缓和这个问题。

2. 精确率和召回率的平衡

   P-R曲线（横坐标召回率，纵坐标精确率）、F1分数

3. 回归模型中的平方根误差（RMSE）可能存在什么情况，怎么解决？

   存在噪声或者离群点时，对RMSE的影响太大，可通过平均绝对百分比误差来缓解离群点带来的绝对误差的影响。

4. ROC曲线、AUC

   ROC曲线（横坐标假阳性率，纵坐标真阳性率），分为正类所需不同的阈值作为截断点，动态调整截断点并在图上画出所在的位置，按阶梯状连接，就得到ROC曲线，而AUC就对应着ROC曲线下的面积，越大却好。

   ROC曲线相较于PR曲线能够有效应对样本不平衡的情况。

5. 在什么场合下用余弦距离而不是欧式距离

   余弦相似度：$cos(A,B) = \frac{A*B}{||A||_{2}||B||_{2}}$，余弦距离：$dist(A,B) = 1 - cos(A,B)$

   余弦距离衡量的是两个向量之间的角度，值域为[0,2]不关心它们的绝对值大小。比如两个句子长短不一，如果它们内容接近，使用词向量做特征的话，余弦距离夹角可能很小，因而相似度高，但欧式距离通常由于长度、词向量的高维度而变得很大。

   在word2vec场景下，向量模长经过归一化，$||X||_{2} = ||Y||_{2} = 1$，此时欧式距离和余弦距离存在单调的关系，即

   $$
   ||X-Y||_{2}^{2} = (X-Y)^{T}(X-Y)\\ = (X^{T}-Y^{T})(X-Y) = 2 - 2X^{T}Y = 2 (1 - cos(A,B))
   $$

6. 余弦距离是否是一个严格定义的距离？

   不是，定义距离需要满足非负性、自达性、对称性、三角形法则

   余弦距离不满足三角形法则，从欧式距离和余弦距离的关系举反例。

   补充：用来衡量两个概率分布的KL散度不满足对称性和三角不等式

7. 在对模型进行过充分的离线评估之后， 为什么还要进行在线A/B测试？  

   * 离线评估无法消除过拟合的影响
   * 离线评估无法还原线上真实场景，比如部分数据丢失、网络延迟
   * 离线评估往往关注的是ROC曲线、 P-R曲线等的改进， 而线上评估可以全面了解该推荐算法带来的用户点击率、 留存时长、 PV访问量等的变化

   进行A/B测试的主要手段是对用户分桶，随机采样出实验组和对照组。

8. 在模型评估过程中， 有哪些主要的验证方法， 它们的优缺点是什么?  

   * 保留样本检验（Hold out）：最常见的方法，将原始样本按照一定比例（8：2）划分为训练集/测试集进行训练、评估。但最终的性能很大程度上取决于随机采的测试集，存在”随机性“

   * K折交叉验证

     将原始数据划分为K个子集，依次选择当前子集为验证集，剩余集合为训练集，取迭代K次的平均性能

   * 自助法（Boostrap）

     上述两种方法都是通过划分数据的方式得到训练集验证集，而自助法可以维持训练集样本规模。

     对于总数为N的样本集合，进行N次有放回的随机采样，得到大小为N的训练集，剩下的没有被采样到的样本作为验证集。当样本数很大的时候，有$\frac{1}{e}$的样本从未被选择，作为验证集。

     一个样本N次都没被抽中的概率为$(1-\frac{1}{n})^{n}$，用公式$\displaystyle \lim_{x \to \infty}(1+\frac{1}{x})^{x} = e$对其求极限得到$\frac{1}{e}$。

9. 超参数的调优方法 

   通过较大的搜索范围和步长，来寻找全局最优值可能的位置。
   
## 三、经典算法

#### SVM

1. 在空间上线性可分的两类点， 分别向SVM分类的超平面上做投影， 这些点在超平面上的投影仍然是线性可分的吗？  
   
   不是，绝对是线性不可分的，从二维平面的中垂线算起。
   
2. 是否存在一组参数使SVM训练误差为0？ 

   存在，

3.  训练误差为0的SVM分类器一定存在吗？  

   存在

4. 加入松弛变量的SVM的训练误差可以为0吗？  

   不可以，因为正则化的影响

#### LR

1. 逻辑回归相比于线性回归， 有何异同？  

   逻辑回归是分类任务，要求输入数据满足伯努利分布，相当于线性回归+sigmoid函数，通过梯度下降求解逻辑回归的极大似然估计函数，达到二分类的目的。

   线性回归是回归问题，要求输入数据满足正态分布，通过最小二乘法最小化平方误差，来达到预测数据的目的。

2. 当使用逻辑回归处理多标签的分类问题时， 有哪些常见做法， 分别应用于哪些场景， 它们之间又有怎样的关系？  

   * 当每个样本仅拥有一个标签时，可以通过多项线性回归，即softmax来分类。
   * 当每个样本拥有多个标签时，可以通过构造N个LR分类器（是狗VS不是狗）来完成多标签的分类

## 十一、Embedding

#### LSA

传统向量空间模型（One-hot，IF-IDF）无法解决一义多词的问题，因为在单词-文档矩阵中不相似的两个文档，可能在语义空间内比较相似。

于是出现了潜在语义分析（Latent Semantic Analysis，LSA），将词文档矩阵（文档与所有词在该文档中出现频次的矩阵）利用SVD分解之后，得到的左奇异向量是词向量，右奇异向量是文档向量。

假设有一词文档矩阵如下：

![](https://img-blog.csdn.net/20180605102300358?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

经过SVD：

![这里写图片描述](https://img-blog.csdn.net/20180605102329453?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

左奇异向量的第一列表示每一个词的出现频繁程度。右奇异向量的第一行表示每一篇文档中的出现词的个数的近似，比如说，T6是0.49，出现了5个词，T2是0.22，出现了2个词。分别取前2维投影到平面上，可以得到

![这里写图片描述](https://img-blog.csdn.net/20180605102428373?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

在图上，每一个红色的点，都表示一个词，每一个蓝色的点，都表示一篇文档，这样我们可以对这些词和文档进行聚类，比如说stock 和 market可以放在一类，按这样聚类出现的效果，可以提取文档集合中的近义词，这样当用户检索文档的时候，是用语义级别（近义词集合）去检索了，而不是之前的词的级别，提高用户体验的同时，减少了检索、存储量。

但是，存在如下问题：

1. SVD计算十分耗时，尤其是文本处理，词和文本数都非常多。
2. 主题值的选取对结果的影响非常大，很难选择合适的k值。
3. LSA得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。

对应的解决方法：

1. 主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。
2. 大部分主题模型的主题个数是凭经验选取的，层次狄利克雷过程（HDP）可以自动选择主题个数。
3. pLSA和隐含狄利克雷分布(LDA)这类基于概率分布的主题模型来替代基于矩阵分解的主题模型。

#### pLSA

文档生成模型：先列出几个主题，然后以一定的概率选择主题，以一定的概率选择这个主题包含的词汇，最终组合成一篇文章。而主题建模的目的就是利用已经产生的文档推测其主题分布。

文档$d$和词$w$是我们得到的样本，可观测得到，所以对于任意一篇文档，其 $P(w_j|d_i)$ 是已知的，设在文章d中生成主题z的概率为$P(z_k|d_m)$， 在选定主题的条件下生成词$w$的概率为$p(w_n|z_k)$， 则给定文章$d$， 生成词$w$的概率可以写成：
$$
P(w_j|d_i) = \sum_zP(w_j|z_k,d_i)P(z_k|d_i)
$$
在这里我们做一个简化， 假设给定主题z的条件下， 生成词w的概率是与特定的文章无关的，从而可以根据大量已知的文档-词项信息$P(w_j|d_i)$，训练出文档-主题$P(z_k|d_i)$ 和主题-词项 $P(w_j|z_k)$，如下公式所示：
$$
P(w_j|d_i) = \sum_zP(w_j|z_k)P(z_k|d_i)
$$
故得到文档$w_j$中每个词$d_i$的生成概率为：
$$
P(d_i,w_j) = P(d_i)P(w_j|d_i) = P(d_i)\sum_{k=1}^K P(w_j|z_k)P(z_k|d_i)
$$
整个文档$w_j$的生成概率用似然函数表示为：
$$
L = \prod_{i}^{M}\prod_{j}^{N}P(d_i,w_j)^{c(d_i,w_j)}
$$
其中，$P(d_i,w_j)$是在文档$j$中出现单词$d_i$的概率，$c(d_i,w_j)$是在文档$j$中单词$w_j$出现的次数。于是，对该似然函数取对数，得：
$$
L = \prod_{i}^{M}\prod_{j}^{N}c(d_i,w_j)log P(d_i,w_j) \\
  = \prod_{i}^{M}\prod_{j}^{N}c(d_i,w_j)log \sum_{k=1}^K P(d_i)P(w_j|z_k)P(z_k|d_i)
$$
由于$c(d_i,w_j)$，$P(d_i)$可根据训练数据求得，而$P(w_j|z_k)$和$P(z_k|d_i)$ 未知待估计，又因为该待估计的参数中含有隐变量$z$，所以使用EM算法来求解上述极大似然函数。

#### LDA

pLSA采用的是频率派思想， 将每篇文章对应的主题分布$P(z_k|d_i)$和每个主题对应的词分布$p(w_i|z_k)$看成确定的未知常数，并可以求解出来； 而LDA采用的是贝叶斯学派的思想，认为
待估计的参数（主题分布和词分布） 不再是一个固定的常数， 而是服从一定分布的随机变量。 这个分布符合一定的先验概率分布（即狄利克雷分布），并且在观察到样本信息之后，可以对先验分布进行修正，从而得到后验分布。 LDA之所以选择狄利克雷分布作为先验分布， 是因为它为多项式分布的共轭先验概率分布，后验概率依然服从狄利克雷分布， 这样做可以为计算带来便利。  

文档生成后，两者都要根据文档去推断其主题分布和词语分布（即两者本质都是为了估计给定文档生成主题，给定主题生成词语的概率），只是用的参数推断方法不同，在pLSA中用极大似然估计的思想去推断两未知的固定参数，而LDA则把这两参数置为随机变量，且加入dirichlet先验，用吉布斯采样估计。

首先随机给定每个单词的主题，然后在其他变量固定的情况下，根据转移概率抽样生成每个单词的新主题。对于每个单词来说，转移概率可以理解为：给定文章中的所有单词以及除自身以外其他所有单词的主题，在此条件下该单词对应为各个新主题的概率。最后，经过反复迭代， 我们可以根据收敛后的采样结果计算主题分布和词分布的期望。

衡量LDA模型在验证集和测试集上的效果，常用的评估指标是困惑度（perplexity）。

#### NNLM

神经网络语言模型

#### word2vec

CBOW和Skip-gram

层次softmax和负采样

#### GloVe

确定滑窗大小得到共现矩阵，通过某种函数构造词向量使得其与共现矩阵得来的概率比值（两个单词i和j哪个跟k更相关）而不是概率本身具有很好的一致性。

尽管GloVe是无监督学习，但存在label，即$log(X_{i,j})$，通过使用AdaGrad的梯度下降法，优化如下目标函数（带权重的均方误差）：
$$
J = \sum_{i,j}^{N}f(X_{i,j})(w_i^T \hat w_j +b_i+\hat b_j-log(X_{i,j}))^2 \\
f(x) =\begin{cases} (x/xmax)^{0.75} &  x<xmax \\  1 & x>=xmax \end{cases}
$$
其中，$X_{i,j}$是共现矩阵$(i,j)$处的值，$w_i^T,\hat w_j$是单词i和j的词向量，$b_i,\hat b_j$是偏差项（标量），$f$是权重函数，$N$是词汇表大小，共现矩阵大小为$N * N$，将$w,\hat w$之和作为最终的词向量（不同初始化的两个词向量矩阵）。

> 为什么损失函数里带权重？
>
> 一个很少出现的词汇携带的信息要比频繁出现的词汇携带的信息要少得多，因此我们需要加上一个$f(X_{i,j})$ 来处理这个问题，使其$Loss$ 能更加关注共现矩阵中出现较频繁的元素。

#### FastText
