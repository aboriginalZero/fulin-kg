## 分片分配策略

在分配每个 PExtent 的分片位置时，ZBS 根据各个 PExtent 的 prefer local 节点所处负载的不同，使用不同的分配策略，具体分片分配策略遵循如下原则：

1. 低负载：① 拓扑安全 ② 本地优先 ③ 局部化。
2. 中负载：① 拓扑安全 ② 本地优先 ③ 低载优先。
3. 高负载：① 拓扑安全 ② 低载优先。

序号越小的原则优先级越高，各个原则含义如下：

* 拓扑安全：不论哪种负载，分片分配策略总是优先满足拓扑安全。若拓扑安全暂时无法满足，允许先分配，后续通过迁移来修复；
* 本地优先：若 prefer local 节点处于健康状态且剩余空间充足，一定将分片分配给它；
* 局部化：若集群拓扑结构、节点健康状态不变且剩余空间充足，prefer local 相同的 PExtent 分片的分配位置是固定不变的；
* 低载优先：若候选节点的拓扑等级一样，将分片分配到负载更低的节点。

用一个3 节点集群来说明，中高负载分别是从 0.5 和 0.6 开始，prefer local = 1，各节点 perf valid = 400 GiB，创建一个 500 GiB 的 2-replica thin volume，并执行 256k 全盘顺序写，perf loc 变化如下：

| step | cid 1 ratio | cid 2 ratio | cid 3 ratio | perf loc       | prefer  cid load | note                                                         |
| ---- | ----------- | ----------- | ----------- | -------------- | ---------------- | ------------------------------------------------------------ |
| 1    | [0, 0.5]    | [0, 0.5]    |             | [1, 2]         | 低               | 遵循局部化分配                                               |
| 2    | [0.5, 0.6]  |             | [0, 0.1]    | [1, 3]         | 中               | 放弃局部化，遵循本地优先 + 拓扑安全 + 低载优先               |
| 3    |             | [0.5, 0.6]  | [0.1, 0.2]  | [2, 3]         | 高               | 放弃本地优先，遵循拓扑安全 + 剩余容量多，此时有 replace = 3，dst = 1 的迁移 |
| 4    |             |             |             | [3, 1], [3, 2] | 高               | cid 3 在负载跟 cid 3 和 4 相同前，总会被分配，perf loc 在 [3, 1]、[3, 2] 交替 |

ZBS 面向客户提供的最小存储对象是 Volume，但属于同一个 Volume 的 PExtent 的 prefer local 并不一定相同。当 PExtent 的 prefer local 为空或是一个不健康的节点，prefer local 的负载没有实际意义，此时根据集群负载决定 PExtent 的分配策略（自然地，本地优先原则被放弃）

### 拓扑安全分配

拓扑安全分配指的是尽可能将分片分配到拓扑距离最远的节点上。在集群服役期间，除了存储节点可能发生物理故障，接入交换机（TOR，每个机箱一个接入交换机）、机柜 UPS 电源（每个机架一个独立电源）都有可能异常，为了提供更高水平的系统可靠性与可用性，需要将分片尽可能分布在物理拓扑的不同分支上。

在集群配置节点拓扑信息之后，就可以据此计算集群中节点之间的拓扑距离。一个节点要么因为没有配置拓扑信息被默认放在可用域中，要么一定被放在某个机箱中，两两节点之间的拓扑距离（topo distance）初始值为 0，依照如下方式计算：

* 若两个节点不在同一个可用域（zone），那么 topo distance - 256；
* 若两个节点不在同一个机架（rack），那么 topo distance - 16；
* 若两个节点不在同一个机柜（brick），那么 topo distance - 1；

分片间的拓扑距离值越小，认为分布越安全。举个例子，若一个 PExtent 已有分片在节点 c1 和 c2，依照拓扑安全分配，第 3 个分片会被分配到跟 c1、c2 拓扑距离最小的节点上，若这样的节点有多个，再按照局部化或低载优先原则来进一步筛选。

### 局部化分配

局部化分配指的是将 prefer local 相同的 PExtent 的分片分配到相同的、固定的节点上，以此来降低节点故障时的影响范围。考虑分片完全均匀分配在每个节点上的场景，此时每个 Volume 中的分片会均匀分散到整个集群，任意一个节点都持有集群中任一 Volume 的部分数据，这样当一个节点异常（节点下线、磁盘异常等）时，集群中所有的 Volume 都会受到影响，且节点故障的影响范围也会随着集群规模的扩张而增大。

局部化分配策略也有明显的劣于大部分分布式系统采用的完全均衡策略。就是在集群节点规模较大的时候，而单一节点上的磁盘能力较低的时候，单一虚拟卷能动员的硬件能力会被局限在有限的节点上而无法发挥整个集群的能力。但是考虑到：

* 即便数据均匀分散到所有节点上，性能上限也会受到网络的限制，技术的发展趋势是磁盘的能力远超过网络的发展能力，现代磁盘就不存在这样的瓶颈（ 2 x NVMe 能提供的 IOPS 和带宽超过 25 G 网卡的上限）；

* 即便是较差的磁盘，存储集群通常也不会只为单一虚拟卷服务，均衡分布带来的 IO 互相干扰的现象对集群整体能力上限更为不利。  


因此 ZBS 选择局部化策略作为空间分配的基础策略。



在实现上，存在多个候选节点跟已有节点的拓扑距离相等时，才会进一步根据局部化分配策略选择下一个分片的分配节点，具体来说，就是按照 ring id 的相对顺序选择跟上一个分片最接近的节点。

ring id 是 zbs 为每个节点添加的一个软件拓扑属性。当一个 ZBS 集群发生拓扑结构变化如 1. node 加入/退出、2. node 跨 zone/rack/brick 移动、3. brick 跨 zone/rack 移动、4. rack 跨 zone 移动时，集群内各节点的 ring id 都会重新生成。ring id 的自动生成规则如下：

1. 对各个 brick 级别的节点列表（每个 brick 中的所有节点构成一个列表）按 topo id 从小到大排序；
2. 遍历集群中每个 rack ：对同一个 rack 内部的 n 个 brick 级别的节点列表先执行粗合并，再执行细合并，得到一个 rack 级别的有序列表；
3. 遍历集群中每个 zone：对同一个 zone 内部的 m 个 rack 级别的节点列表先执行粗合并，再执行细合并，得到一个 zone 级别的有序列表；
4. 对 2 个 zone 级别的有序列表执行细合并（只有优先可用域/次级可用域 2 个节点列表，所以不用执行粗合并），得到一个 cluster 级别的节点列表，此时，每个节点在该列表中的位置就是它的 ring_id。

其中，粗合并指的是将给定的 n 个节点列表合并成 2 个长度尽可能接近的节点列表 lhs 和 rhs，目的是让副本分配也会落在长列表中的尾节点上，充分利用每个节点容量，保证整体副本分配尽可能均匀。细合并指的是将给定的 2 个节点列表划分成 k = min(len(lhs), len(rhs)) 并顺序按组配对，目的是让副本分配尽可能局部化，使得拓扑变动引起的副本迁移能够控制在与 k 正相关的节点规模内。

当集群拓扑结构、节点健康状态不变且剩余空间充足时，prefer local 相同的 PExtent 分片得到的期望局部化分片列表是相同且固定的，这也意味着 prefer local 不同的 PExtent 的分片位置的重叠概率是很低的，不同 Volume 之间占用的物理资源相对独立，这除了可以缩小故障影响范围，还便于按需调整各个 Volume 的 Qos 策略，让集群性能随节点数线性增长。

> TODO 补充访问感知，不同存储对象的访问模式会影响到接入点，进而影响到 prefer local

## 分片迁移策略

设计目标是避免反复迁移、争取一步到位

### 节点移除迁移

每 4s 一次，按照 recover 的选取策略进行。

### 均匀卷迁移

因为均匀卷的数量相对较少，不关注负载情况，总是尝试执行以 ① 拓扑修复 ② 数量均衡 为目的的迁移



### 非均匀卷迁移

非均匀卷，也就是大部分卷的迁移，依照集群负载不同，执行不同的迁移策略：

1. 低负载：① 局部化修复 ② 拓扑修复 ③ 本地化修复
2. 中负载：① 拓扑修复 ② 本地化修复 ③ 容量均衡
3. 高负载：① 拓扑修复 ②  容量均衡
4. 超高负载：① 容量均衡



分片分配策略中，在集群进入中负载才放弃局部化，进入高负载才放弃本地优先，迁移策略也需要满足这个大的原则，但不同的是，迁移策略中需要有一个弹性边界，用以避免节点容量在负载转换边界时因小流量 IO 流入出现迁移策略不断转化进而导致分片反复迁移。

弹性边界是针对节点而非集群而言，以 cap layer 的默认参数 medium load = 75%、 high load = 85%、very high load = 95%、relaxation = 5% 为例，来说明弹性边界的策略：

1. 当集群处于低负载 [0, 75)，集群中处于严格低负载 [0, 70) 的节点可以保证相关数据块的分片分布满足局部化，但处于低中边界 [70, 75) 的节点无法保证；
2. 当集群处于中负载 [75, 85)，集群中处于严格中负载 [75, 80) 的节点可以保证相关数据块存在本地分片，但处于中高边界 [80, 85) 的节点无法保证；
3. 当集群处于高负载 [85, 95)，集群中处于严格高负载 [90, 95) 的节点可以保证相关数据块的分片分布满足拓扑安全（包含双活域间分片 2 ：1 的特点），但处于高超高边界 [85, 95) 的节点无法保证；

#### 局部化迁移

局部化迁移（migrate for localization） 指的是：当集群处于低负载，若集群拓扑结构、节点健康状态、数据块的 prefer local 发生变化，非均匀卷的数据块副本将会通过局部化修复迁移来达到新的期望局部化分布。

通过以下 3 个步骤选取分片放置节点构成的分布称为期望局部化分布：

1. 若 prefer local 处于健康状态且可以容纳新分片，那么 prefer local 作为第 1 节点，否则选取负载最低的节点作为第 1 节点；
2. 从剩余节点中选取与第 1 节点拓扑距离最近的节点作为第 2 节点，若存在多个，那么按照 ring id 的相对顺序从拓扑环上选取与第 1 节点最近的节点作为第 2 节点；
3. 与步骤 2 同理，从剩余节点中选取与第 k - 1 节点拓扑距离最近且在拓扑环上最近的节点作为第 k 节点，直到选出 n 个分片放置节点构成期望局部化分布。

对于给定 prefer local 的每个数据块而言，当集群拓扑结构和节点健康状态不变时，它的期望局部化分布也一定保持不变。



#### 本地化迁移

#### 拓扑修复迁移

#### 容量均衡迁移



## 分片恢复策略

## 分片回收策略





一次 recover 完成流程描述（主要在 recover handler 里的逻辑）

一次 agile recover 的流程



常用数值的计算



内部 IO 自适应调节

做不完就少发，cmd pending 时间过长，



https://docs.google.com/document/d/1CTxMn6pyajujO7u7sbsggz3gbDV7YtDWKgH8NRgB5ZA/edit?tab=t.0

https://docs.google.com/document/d/17H6WVHB3fWr_JnyNutRkycpXSIZbWKujS6oybFZhR64/edit?tab=t.0



